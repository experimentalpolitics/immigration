---
title: "Letter to Reviewer"
subtitle: "Reliable Sources? Correcting Misinformation in Polarized Media Environments"
fontsize: 12pt
geometry: margin=1in
output: 
  pdf_document:
    number_sections: false
    template: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```


`We thank the reviewer and editor for engaging with our work and providing us with such helpful feedback. We have made a number of changes in response to the comments, which in our view, have considerably strengthened our manuscript. Below, we respond to the reviewer's comments in detail and summarize the resulting revisions.`

1.	My main concern is with the methodology and thus the conclusions of the paper. Specifically, I failed to understand how the authors arrive at such conclusions without having a clear and identifiable set of pretreatment questions that map precisely to the post-treatment outcomes. As I understand, the logic behind the experiment is to implement some pretest-posttest to manipulate whether participants are allowed to chose Fox or MSNBC; however, there are no pretreatment questions that could serve as a baseline. If, however, the objective of the paper is to test the manipulation without a baseline, then it needs to refine and clarify the language of the research design that is highly confusing at the very least.  If the objective of the experiment is to test the manipulation (freely to choose or assigned to a media outlet), the distribution between treatment and control groups are everything but balanced (I assume that the table in Appendix A describes the sample after randomization), which makes me question if the differences that exist between treatment conditions will on average disappear after randomization, the distributions are all over the place. Simply by looking at table I and without more information, it is impossible to ascertain if the randomization worked or not as it is presented; the data suggests that there are significant problems. For example, given that there seem to be substantial differences between groups, how can we be sure that there is about a 20 to 30 pp difference?

> `We agree that our discussion of the quasi pretest-posttest design needed clarification and made multiple revisions to that effect on pages...`

> `Regarding the issue of balance between our treatment and control conditions, we want to point out that per random assignment, our treatment conditions are balanced in expectation. However, our presentation of the distributions across treatment and control groups in a table was confusing, so we replaced it with a graphical comparison of distributions across treatment groups. We now supplement this graphical presentation with a multinomial logistic regression of assignment to each of the treatment groups (control group is reference category).`

- `DONE:` Improve balance checks using figures instead of table. 
- `DONE:` Add model predicting treatment condition
- `TODO:` Cite Hopkins in main text
- `TODO:` Cite Clifford on quasi pretest-posttest designs
- `TODO:` Show that many people get it wrong in the control group.
- `TODO:` Explain that perceived imbalance was due to our presentation...
- `TODO:` Mention that even if there is a certain level of imbalance, we are including controls for various pre-treatment covariates, so it shouldn't be a problem.

2. The authors also need to discuss the flaws inherent with MTurk data and the use of VPNs outside the U.S. that may significantly bias the quality of the data. The authors need to present a more balanced view of the data and address the concerns that many researchers have with such data. 

> `This is a very important point. We elaborated on the data quality of MTurk samples as well as our measures to block VPNs in the beginning of our results section (p. 11).`

<!-- - `DONE:` Check language from other journals. -->
<!-- - `DONE:` Add discussion in results section. -->

3. The authors also need to discuss why they decided to implement OLS when the outcome variable is dichotomous, as indicated on page 11.

> `We added a discussion of our modeling choices in the results section and explained why we implemented a linear probability model instead of a logit/probit when the outcome vatiable is dichotomous (now on p. 12). In addition, we replicated all analyses with dichotomous outcome measures in the paper using logistic regression (see Appendix B III). The substantive results are consistent across model specifications.`

<!-- - `DONE:` Run logit models. -->
<!-- - `DONE:` Add logit tables to appendix. -->
<!-- - `DONE:` Add language in main text describing linear probability model. Cite? -->
<!-- - `DONE:` Add reference in main text to logistic regression results. -->

4. My last concern has to do with how credible it is for Fox News to publish such a positive article on immigration. Are there any potential issues with such a treatment that may not be believable and/or trustworthy?

> `This is another important point and we agree with the reviewer that the credibility of the Fox News article is a potential concern. Luckily, we included a set of items asking respondents to evaluate the news article after receiving the treatment (asking to what extent they would describe the article as balanced, accurate, fair, good, friendly, and American; see the relevant part of the questionnaire in Appendix D II). We now added a summary of these evaluations conditional on the source and pre-treatment media preference in Appendix A IV. While we didn't ask respondents directly whether they consider the article to be believable or authentic, the results presented there suggest that they did.`

> `ACTE results play a role here as well!`

<!-- - `DONE:` Check evaluation of news articles in the appendix. -->
<!-- - `DONE:` Add discussion / footnote in main text. -->
<!-- - `DONE:` Make connection to ACTE results as well. -->

<!-- ## Other stuff -->

<!-- - `DONE:` update code: condint vs. confint_tidy -->
